{"cells":[{"source":"Here we are going to use an LLM and Langchain to perform a complex task called PAL (Program-Aided Language Models).\n\nThis is a method that uses  LLMs to read natural language problems and generate programs as the intermediate reasoning steps. Coined, program-aided language models (PAL), it differs from chain-of-thought prompting in that instead of using free-form text to obtain a solution it offloads the solution step to a programmatic runtime such as a Python interpreter.\n\nLet's look at an example using LangChain and OpenAI GPT-3. We are interested in developing a simple application that's able to interpret the question being asked and provide an answer by leveraging the Python interpreter.\n\nSpecifically, we are interested in creating a functionality that allows the use of the LLM to answer questions that require reasoning, tasks related to penguin data.. We will provide the LLM with a prompt that includes a few examples.\n\nThese are the imports we need","metadata":{},"cell_type":"markdown","id":"d46f4bef-4f6a-44ad-a5fa-7a49bee06e98"},{"source":"import openai\nimport os\nimport IPython\nfrom dotenv import load_dotenv\nfrom langchain.llms import OpenAI","metadata":{"executionCancelledAt":null,"executionTime":2241,"lastExecutedAt":1716310982326,"lastExecutedByKernel":"0bda35a6-5afd-4212-ab1c-93454494b0a5","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import openai\nimport os\nimport IPython\nfrom dotenv import load_dotenv\nfrom langchain.llms import OpenAI"},"cell_type":"code","id":"c4d58aea-6509-438d-8169-e6268623890c","outputs":[],"execution_count":1},{"source":"let's first configure a few things:","metadata":{},"cell_type":"markdown","id":"b02882ab-81c8-48c6-a651-6c9b77a4d4de"},{"source":"load_dotenv()\n\n# API configuration\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# for Langchain\nos.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")","metadata":{"executionCancelledAt":null,"executionTime":16,"lastExecutedAt":1716311337183,"lastExecutedByKernel":"0bda35a6-5afd-4212-ab1c-93454494b0a5","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"load_dotenv()\n\n# API configuration\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n# for Langchain\nos.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"},"cell_type":"code","id":"439b2e06-75b4-4819-b72b-1c25e1772c8c","outputs":[],"execution_count":2},{"source":"Initialize the Language Model:","metadata":{},"cell_type":"markdown","id":"cd4a9b1a-32c6-4bda-b358-4d61e5d4baf2"},{"source":"#llm = OpenAI(model_name = \"text-davinci-003\", temperature=0)\nllm = OpenAI(model_name = \"gpt-3.5-turbo-instruct\", temperature=0)","metadata":{"executionCancelledAt":null,"executionTime":98,"lastExecutedAt":1716312173547,"lastExecutedByKernel":"0bda35a6-5afd-4212-ab1c-93454494b0a5","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"#llm = OpenAI(model_name = \"text-davinci-003\", temperature=0)\nllm = OpenAI(model_name = \"gpt-3.5-turbo-instruct\", temperature=0)"},"cell_type":"code","id":"f0d378ba-7975-4c23-a339-a61343eeeb71","outputs":[],"execution_count":8},{"source":"Here, an instance of the OpenAI language model is created with the model name ~'text-davinci-003'~ 'gpt-3.5-turbo-instruct' and a temperature of 0. The temperature parameter controls the randomness of the model's output.\n\nSetup prompt + question:","metadata":{},"cell_type":"markdown","id":"de43a48e-4800-4468-b78d-c05234d70f22"},{"source":"question = \"Which is the heaviest penguin?\"\n\nPENGUIN_PROMPT ='''\n\"\"\"\n... (omitting the detailed prompt for brevity)\n\"\"\"\n'''.strip() + '\\n' ","metadata":{"executionCancelledAt":null,"executionTime":11,"lastExecutedAt":1716311927617,"lastExecutedByKernel":"0bda35a6-5afd-4212-ab1c-93454494b0a5","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"question = \"Which is the heaviest penguin?\"\n\nPENGUIN_PROMPT ='''\n\"\"\"\n... (omitting the detailed prompt for brevity)\n\"\"\"\n'''.strip() + '\\n' "},"cell_type":"code","id":"51adb511-5554-47d7-a9ee-e4a602953c3b","outputs":[],"execution_count":5},{"source":"Run the LLM:","metadata":{},"cell_type":"markdown","id":"b63b5622-895f-4e99-92e3-2bb9f16cbff2"},{"source":"llm_output = llm(PENGUIN_PROMPT.format(question = question))\nprint(llm_output)","metadata":{"executionCancelledAt":null,"executionTime":515,"lastExecutedAt":1716313072525,"lastExecutedByKernel":"0bda35a6-5afd-4212-ab1c-93454494b0a5","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"llm_output = llm(PENGUIN_PROMPT.format(question = question))\nprint(llm_output)","outputsMetadata":{"0":{"height":122,"type":"stream"}}},"cell_type":"code","id":"8682c255-2aaa-4013-bdce-26ed07c1c1ff","outputs":[{"output_type":"stream","name":"stdout","text":"\n\ndef main():\n    # TODO: implement this function\n    pass\n"}],"execution_count":15},{"source":"This will output the following:","metadata":{},"cell_type":"markdown","id":"ddd25e61-deb5-44b8-b67d-9f3f563e29cf"},{"source":"# Put the penguins into a list.\n\npenguins = []\npenguins.append(('Louis', 7, 50, 11))\npenguins.append(('Bernard', 5, 80, 13))\npenguins.append(('Vincent', 9, 60, 11))\npenguins.append(('Gwen', 8, 70, 15))\n\n# Sort penguins by weight.\npenguins_by_weight = sorted(penguins, key = lambda x: x[3], reverse=True)\n\n#Get the heaviest penguin's name\nheaviest_penguin_name = penguins_by_weight[0][0]\nanswer = heaviest_penguin_name","metadata":{"executionCancelledAt":null,"executionTime":16,"lastExecutedAt":1716312916854,"lastExecutedByKernel":"0bda35a6-5afd-4212-ab1c-93454494b0a5","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Put the penguins into a list.\n\npenguins = []\npenguins.append(('Louis', 7, 50, 11))\npenguins.append(('Bernard', 5, 80, 13))\npenguins.append(('Vincent', 9, 60, 11))\npenguins.append(('Gwen', 8, 70, 15))\n\n# Sort penguins by weight.\npenguins_by_weight = sorted(penguins, key = lambda x: x[3], reverse=True)\n\n#Get the heaviest penguin's name\nheaviest_penguin_name = penguins_by_weight[0][0]\nanswer = heaviest_penguin_name"},"cell_type":"code","id":"f1f7e006-d8a6-49f4-a7b0-6a6575392867","outputs":[],"execution_count":12},{"source":"The contents of llm_output are a Python code snippet. Below, the exec command is used to execute this Python code snippet.","metadata":{},"cell_type":"markdown","id":"6726c2d5-a1e0-43c0-8ad0-471ccdae01d5"},{"source":"exec(llm_output)\nprint(answer)","metadata":{"executionCancelledAt":null,"executionTime":14,"lastExecutedAt":1716313094632,"lastExecutedByKernel":"0bda35a6-5afd-4212-ab1c-93454494b0a5","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"exec(llm_output)\nprint(answer)","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"cell_type":"code","id":"1530af19-20a9-4d23-8761-2b7f08a080ac","outputs":[{"output_type":"stream","name":"stdout","text":"Gwen\n"}],"execution_count":16}],"metadata":{"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"editor":"DataLab"},"nbformat":4,"nbformat_minor":5}